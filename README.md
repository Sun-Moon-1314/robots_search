# 机器人迷宫寻路强化学习项目

本项目实现了一个基于强化学习的机器人迷宫寻路系统，使用PyBullet进行物理模拟，通过Stable Baselines 3实现强化学习算法。

## 项目结构
```bash
robots_projects/
├── config/                     # 配置文件目录
│   ├── maze_search/            # 迷宫搜索相关配置
│   │   └── __init__.py
│   ├── __init__.py
│   ├── curriculum_config.py    # 课程学习配置
│   └── default_config.py       # 默认配置
│
├── envs/                       # 环境模块目录
│   ├── mujoco/                 # MuJoCo相关环境
│   ├── pybullet/               # PyBullet相关代码
│   │   ├── __init__.py
│   │   ├── maze_builder.py     # 迷宫构建器
│   │   └── maze_search.py      # 迷宫搜索环境
│   └── __init__.py
│
├── evaluation/                 # 评估模块
│   ├── __init__.py
│   └── evaluator.py            # 模型评估器
│
├── reward_function/            # 奖励函数模块
│   ├── __init__.py
│   └── maze_search_reward.py   # 迷宫搜索的奖励函数
│
├── training/                   # 训练模块
│   ├── __init__.py
│   ├── curriculum.py           # 课程学习实现
│   └── trainer.py              # 训练器
│
├── utils/                      # 工具函数
│   ├── __init__.py
│   ├── logger.py               # 日志工具
│   └── visualization.py        # 可视化工具
│
├── main.py                     # 主入口脚本
└── README.md                   # 项目说明
```


## 环境要求

- Python 3.8+
- PyBullet
- Gymnasium
- Stable Baselines 3
- NumPy
- Matplotlib
- Pandas
- Seaborn

## 安装依赖

```bash
pip install -r requirements.txt
```
## 使用方法
### 测试环境
```bash
python main.py test [--episodes N] [--no-render] [--verbose]
```
### 训练模型
```bash
python main.py train [--algorithm {SAC,PPO,A2C}] [--timesteps N] [--render] [--verbose]
```
### 使用课程学习训练
```
python main.py curriculum [--render] [--resume] [--verbose]
```
### 评估模型
```
python main.py eval --model MODEL_PATH [--episodes N] [--no-render] [--verbose]
```

## 课程学习
### 本项目实现了三阶段课程学习：

- 平衡学习：机器人学习在迷宫中保持平衡
- 短距离导航：机器人学习在保持平衡的同时导航到短距离目标
- 长距离导航：机器人学习在保持平衡的同时导航到长距离目标

## 奖励函数
*奖励函数综合考虑了以下因素*：

- close: 接近目标阈值(米)：表示机器人与目标（如球）之间的距离小于此值时，视为接近目标。通常用于调整距离奖励或惩罚的逻辑。值越小，要求机器人更靠近目标才算“接近”，难度越高；值越大，机器人离目标较远时也可能获得奖励，难度较低。

- fallen: 摔倒阈值(弧度)：用于判定机器人是否摔倒的倾斜角度阈值，约为23度。值越大，摔倒判定越宽松，机器人需要更大的倾斜角度才会被判定为摔倒（越不容易摔倒）；值越小，摔倒判定越严格，机器人稍微倾斜就可能被判定为摔倒（越容易摔倒）。

- warning: 摔倒预警阈值(弧度)：用于在机器人接近摔倒状态时给予警告惩罚的倾斜角度阈值，约为28.6度。通常小于或接近‘fallen’值，用于在摔倒前逐步增加惩罚。值越大，预警范围越宽，机器人更早会受到警告惩罚；值越小，预警范围越窄，只有接近摔倒时才会有惩罚。

- tilt_threshold: 倾斜阈值(弧度)：用于特定场景（如接近目标时的倾斜惩罚）的倾斜角度阈值，约为11.5度。值越大，对倾斜的容忍度越高，机器人倾斜更多才会被惩罚；值越小，对倾斜的容忍度越低，稍微倾斜就可能受到惩罚。

- angle_threshold: 侧面目标角度阈值(弧度)：用于判断目标（如球）是否在机器人侧面或偏离方向的角度阈值，约为28.6度。常用于方向探索奖励。值越大，目标偏离机器人朝向的角度范围越宽，奖励触发更容易；值越小，要求目标更接近机器人正前方，奖励触发更难。

- max_angle_reward: 最大角度奖励：方向探索或角度调整时的最大奖励值，用于限制角度奖励的上限。值越大，角度调整的奖励上限越高，可能会更鼓励转向行为；值越小，角度奖励上限越低，转向行为的激励较弱。

- angle_reward_factor: 角度变化奖励因子：用于放大或缩小角度变化带来的奖励或惩罚的影响。值越大，角度变化对奖励的影响越显著，鼓励快速调整方向；值越小，角度变化的影响越小，方向调整的奖励或惩罚不明显。

- direction_exploration_scale: 方向探索缩放系数：用于调整方向探索奖励的整体幅度。值越大，方向探索奖励或惩罚的幅度越大，鼓励探索新方向；值越小，方向探索奖励或惩罚的幅度越小，探索行为的激励较弱。

- max_rate: 角速度最大值(弧度/秒)：用于归一化角速度（如roll_rate和pitch_rate）的参考值，确保角速度奖励或惩罚在合理范围内。值越大，归一化后的角速度值越小，惩罚或奖励对角速度的敏感度降低；值越小，归一化后的角速度值越大，惩罚或奖励对角速度的敏感度提高。

- 距离奖励：接近目标的奖励
- 平衡奖励：保持平衡的奖励
- 碰撞惩罚：碰撞墙壁的惩罚
- 能量效率：最小化动作幅度的奖励
- 摔倒惩罚：机器人倾斜过大的惩罚
- 速度奖励：保持前进速度的奖励
- 超时惩罚：回合超时的惩罚

## 许可证
**MIT**
## 总结

以上是我根据您的项目结构图设计的模块化代码结构。这种设计有以下优点：

1. **模块化**：将代码分解为独立的模块，每个模块负责特定的功能
2. **可维护性**：每个文件都有明确的职责，便于维护和更新
3. **可扩展性**：可以轻松添加新功能或修改现有功能
4. **可读性**：代码结构清晰，易于理解
5. **可复用性**：模块化设计使代码可以在其他项目中复用

这个设计实现了您所需的所有功能，包括：
- 迷宫环境模拟
- 强化学习训练
- 课程学习
- 模型评估
- 可视化工具

您可以根据实际需求进一步调整代码细节。
