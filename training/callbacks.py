# -*- coding: utf-8 -*-
"""
@File    : curriculum_callbacks.py
@Author  : zhangjian
@Desc    : è¯¾ç¨‹å­¦ä¹ çš„å›è°ƒå‡½æ•°
"""

import os
import time
import numpy as np
from typing import Dict, Any, Optional
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.vec_env import VecEnv, sync_envs_normalization
from path_config import *
from custom_design.custom_common import get_config
from evaluation.evaluator import get_base_env


class CurriculumEvalCallback(EvalCallback):
    """å¢å¼ºç‰ˆè¯¾ç¨‹å­¦ä¹ è¯„ä¼°å›è°ƒï¼Œæ”¯æŒå¥–åŠ±é˜ˆå€¼å’Œæ€§èƒ½æŒ‡æ ‡ä¸¤ç§åˆ¤æ–­æ–¹å¼"""

    def __init__(
            self,
            eval_env: VecEnv,
            phase: int = 1,
            performance_thresholds: Optional[dict] = None,
            callback_on_new_best: Optional[BaseCallback] = None,
            n_eval_episodes: int = 5,
            eval_freq: int = 10000,
            log_path: Optional[str] = None,
            best_model_save_path: Optional[str] = None,
            deterministic: bool = True,
            render: bool = False,
            verbose: int = 1,
            warn: bool = True,
            early_stop_patience: int = 5,
            min_delta: float = 0.1,
            std_threshold_ratio: float = 0.5,
            distance_threshold: float = 0.5,
            tilt_threshold: float = 0.3,
            check_direction: bool = False,
            success_rate_threshold: float = 0.9,
            success_window_size: int = 5,
            eval_window_size: int = 100,  # æ–°å¢å‚æ•°ï¼šè¯„ä¼°çª—å£å¤§å°ï¼Œç”¨äºå¹³å‡å€¼è®¡ç®—
            max_steps_threshold: int = 70,  # æ–°å¢å‚æ•°ï¼šæˆåŠŸä»»åŠ¡çš„æœ€å¤§æ­¥æ•°é˜ˆå€¼
    ):
        super().__init__(
            eval_env=eval_env,
            callback_on_new_best=callback_on_new_best,
            n_eval_episodes=n_eval_episodes,
            eval_freq=eval_freq,
            log_path=log_path,
            best_model_save_path=best_model_save_path,
            deterministic=deterministic,
            render=render,
            verbose=verbose,
            warn=warn,
        )
        self.phase = phase
        self.performance_thresholds = performance_thresholds
        self.phase_complete = False
        self.std_threshold_ratio = std_threshold_ratio

        # æ—©åœç›¸å…³å‚æ•°
        self.early_stop_patience = early_stop_patience
        self.min_delta = min_delta
        self.no_improvement_count = 0
        self.best_mean_reward_for_early_stop = -np.inf

        # çŠ¶æ€æ¡ä»¶æ—©åœå‚æ•°
        self.distance_threshold = distance_threshold
        self.tilt_threshold = tilt_threshold
        self.check_direction = check_direction

        # æˆåŠŸç‡æ—©åœå‚æ•°
        self.success_rate_threshold = success_rate_threshold
        self.success_window_size = success_window_size
        self.success_history = []
        self.success_rate = 0.0
        self.success_early_stop_count = 0
        # è¯„ä¼°çª—å£å‚æ•°
        self.eval_window_size = eval_window_size  # è¯„ä¼°çª—å£å¤§å°ï¼Œç”¨äºå¹³å‡å€¼è®¡ç®—
        # æ–°å¢ï¼šæˆåŠŸä»»åŠ¡æœ€å¤§æ­¥æ•°é˜ˆå€¼
        self.max_steps_threshold = max_steps_threshold
        self.adjusted_max_steps_threshold = max_steps_threshold  # åŠ¨æ€è°ƒæ•´åçš„é˜ˆå€¼
        # è¯„ä¼°å†å²
        self.evaluation_history = []
        self.performance_metrics = {}
        self.performance_history = []

    def _check_state_conditions(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³çŠ¶æ€æ¡ä»¶ä»¥è§¦å‘æ—©åœ"""
        if not self.performance_metrics:
            return False

        distance_ok = (
                'distance_to_target' in self.performance_metrics and
                self.performance_metrics['distance_to_target'] <= self.distance_threshold
        )

        tilt_ok = (
                'tilt_angle' in self.performance_metrics and
                self.performance_metrics['tilt_angle'] <= self.tilt_threshold
        )

        direction_ok = True
        if self.check_direction:
            direction_ok = (
                    'movement_alignment' in self.performance_metrics and
                    self.performance_metrics['movement_alignment'] >= 0.5
            )

        steps_ok = (
                'current_step' in self.performance_metrics and
                self.performance_metrics['current_step'] <= self.adjusted_max_steps_threshold
        )

        all_conditions_met = distance_ok and tilt_ok and direction_ok and steps_ok

        if self.verbose > 1:
            print(f"æ—©åœçŠ¶æ€æ¡ä»¶æ£€æŸ¥ï¼š")
            print(
                f"  è·ç¦»æ¡ä»¶: {distance_ok} ({self.performance_metrics.get('distance_to_target', 'N/A'):.2f} <= {self.distance_threshold:.2f})")
            print(
                f"  å€¾æ–œè§’æ¡ä»¶: {tilt_ok} ({self.performance_metrics.get('tilt_angle', 'N/A'):.2f} <= {self.tilt_threshold:.2f})")
            if self.check_direction:
                print(
                    f"  æ–¹å‘æ¡ä»¶: {direction_ok} ({self.performance_metrics.get('movement_alignment', 'N/A'):.2f} >= 0.5)")
            print(
                f"  æ­¥æ•°æ¡ä»¶: {steps_ok} ({self.performance_metrics.get('current_step', 'N/A'):.2f} <= {self.adjusted_max_steps_threshold:.2f})")

        return all_conditions_met

    def _check_success_rate_condition(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³æˆåŠŸç‡æ¡ä»¶ä»¥è§¦å‘æ—©åœï¼ŒåŸºäºçª—å£å†…å¹³å‡å€¼"""
        if len(self.success_history) < self.eval_window_size:
            if self.verbose > 1:
                print(f"è¯„ä¼°æ¬¡æ•°ä¸è¶³ {self.eval_window_size}ï¼Œå½“å‰ä¸º {len(self.success_history)}ï¼Œæš‚ä¸è§¦å‘æ—©åœ")
            return False

        # è·å–çª—å£å†…çš„æˆåŠŸç‡å†å²è®°å½•
        window_success_history = self.success_history[-self.eval_window_size:]
        total_episodes = len(window_success_history)
        successful_episodes = sum(window_success_history)
        self.success_rate = successful_episodes / total_episodes if total_episodes > 0 else 0.0

        success_condition_met = self.success_rate >= self.success_rate_threshold

        if success_condition_met:
            self.success_early_stop_count += 1
            if self.verbose > 0:
                print(f"æˆåŠŸç‡æ¡ä»¶æ»¡è¶³: {self.success_rate:.2%} >= {self.success_rate_threshold:.2%}, "
                      f"è¿ç»­æ»¡è¶³æ¬¡æ•°: {self.success_early_stop_count}")
        else:
            self.success_early_stop_count = 0
            if self.verbose > 0:
                print(f"æˆåŠŸç‡æ¡ä»¶æœªæ»¡è¶³: {self.success_rate:.2%} < {self.success_rate_threshold:.2%}, é‡ç½®è®¡æ•°å™¨")

        return self.success_early_stop_count >= 3  # ä»ç„¶ä¿ç•™è¿ç»­ 3 æ¬¡çš„æ¡ä»¶ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´

    def _custom_evaluate(self):
        """è‡ªå®šä¹‰è¯„ä¼°æ–¹æ³•ï¼Œæ”¶é›†æ€§èƒ½æŒ‡æ ‡å’ŒæˆåŠŸç‡"""
        # åŠ¨æ€è°ƒæ•´ max_steps_thresholdï¼Œæ£€æŸ¥ç¯å¢ƒä¸­æ˜¯å¦æœ‰ steps_per_grid å±æ€§
        if hasattr(self.training_env, 'steps_per_grid'):
            self.adjusted_max_steps_threshold = min(self.max_steps_threshold, self.training_env.steps_per_grid)
            if self.verbose > 0:
                print(
                    f"åŠ¨æ€è°ƒæ•´æ­¥æ•°é˜ˆå€¼: ä½¿ç”¨ {self.adjusted_max_steps_threshold} (å– {self.max_steps_threshold} å’Œç¯å¢ƒ steps_per_grid {self.training_env.steps_per_grid} çš„æœ€å°å€¼)")
        else:
            self.adjusted_max_steps_threshold = self.max_steps_threshold
            if self.verbose > 0:
                print(f"ç¯å¢ƒä¸­æ—  steps_per_grid å±æ€§ï¼Œä½¿ç”¨é»˜è®¤æ­¥æ•°é˜ˆå€¼: {self.adjusted_max_steps_threshold}")

        episode_performance_metrics = {
            "phase": None,
            "current_step": [],
            "tilt_angle": [],
            "distance_to_target": [],
            "movement_alignment": [],
            "direction_exploration": []
        }
        episode_rewards, episode_lengths = [], []
        episode_successes = []
        eval_env = self.training_env
        eval_env.reset()
        if hasattr(eval_env, 'prev_angle_diff'):
            eval_env.prev_angle_diff = None

        for i in range(self.n_eval_episodes):
            obs = eval_env.reset()
            if hasattr(eval_env, 'prev_angle_diff'):
                eval_env.prev_angle_diff = None

            done, state = False, None
            episode_reward = 0.0
            episode_length = 0
            episode_metrics = {
                "current_step": 0,
                "tilt_angle": [],
                "distance_to_target": [],
                "movement_alignment": [],
                "direction_exploration": []
            }
            episode_success = False

            while not done:
                action, state = self.model.predict(obs, state=state, deterministic=self.deterministic)
                obs, reward, done, info = eval_env.step(action)

                # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                if hasattr(eval_env, "get_performance_metrics"):
                    metrics = eval_env.get_performance_metrics()
                    for key, value in metrics.items():
                        if key in episode_metrics:
                            if isinstance(value, (list, tuple)):
                                episode_metrics[key].extend(value)
                            else:
                                episode_metrics[key].append(value)

                # æ‰‹åŠ¨æå–å…³é”®æŒ‡æ ‡ - é€‚é…28ç»´è§‚å¯Ÿç©ºé—´ï¼Œå¤„ç†æ‰¹é‡æ•°æ®
                current_obs = obs[0] if len(obs.shape) > 1 and obs.shape[0] == 1 else obs
                if len(current_obs) >= 28:
                    roll, pitch = current_obs[5], current_obs[6]
                    tilt_angle = np.sqrt(roll ** 2 + pitch ** 2)
                    episode_metrics["tilt_angle"].append(tilt_angle)
                    dx, dy = current_obs[3], current_obs[4]
                    distance = np.sqrt(dx ** 2 + dy ** 2)
                    episode_metrics["distance_to_target"].append(distance)
                    movement_alignment = current_obs[27]
                    episode_metrics["movement_alignment"].append(movement_alignment)
                    yaw = current_obs[2]
                    target_angle = np.arctan2(dy, dx)
                    angle_diff = np.arctan2(np.sin(target_angle - yaw), np.cos(target_angle - yaw))
                    if hasattr(eval_env, 'prev_angle_diff') and eval_env.prev_angle_diff is not None:
                        angle_diff_change = abs(angle_diff) - abs(eval_env.prev_angle_diff)
                        episode_metrics["direction_exploration"].append(-angle_diff_change)
                    if hasattr(eval_env, 'prev_angle_diff'):
                        eval_env.prev_angle_diff = angle_diff

                episode_reward += reward[0] if isinstance(reward, (list, tuple, np.ndarray)) else reward
                episode_length += 1
                if isinstance(done, (list, tuple, np.ndarray)):
                    done = any(done)

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

            # å›åˆç»“æŸåï¼ŒåŸºäºå¹³å‡æ€§èƒ½æŒ‡æ ‡åˆ¤æ–­æ˜¯å¦æˆåŠŸ
            avg_metrics = {}
            for key, values in episode_metrics.items():
                if values:
                    avg_metrics[key] = np.mean(values)

            # æˆåŠŸæ¡ä»¶ï¼šè·ç¦»ã€å€¾æ–œè§’ã€å¯é€‰çš„æ–¹å‘å¯¹é½ä»¥åŠæ­¥æ•°å‡æ»¡è¶³é˜ˆå€¼
            distance_ok = 'distance_to_target' in avg_metrics and avg_metrics[
                'distance_to_target'] <= self.distance_threshold
            tilt_ok = 'tilt_angle' in avg_metrics and avg_metrics['tilt_angle'] <= self.tilt_threshold
            direction_ok = True
            if self.check_direction:
                direction_ok = 'movement_alignment' in avg_metrics and avg_metrics['movement_alignment'] >= 0.5
            steps_ok = episode_length <= self.adjusted_max_steps_threshold  # æ–°å¢æ­¥æ•°æ¡ä»¶

            episode_success = distance_ok and tilt_ok and direction_ok and steps_ok
            episode_successes.append(episode_success)

            if self.verbose > 1:
                print(f"å›åˆ {i + 1}/{self.n_eval_episodes} æˆåŠŸåˆ¤æ–­ï¼š")
                print(
                    f"  è·ç¦»æ¡ä»¶: {distance_ok} ({avg_metrics.get('distance_to_target', 'N/A'):.2f} <= {self.distance_threshold:.2f})")
                print(
                    f"  å€¾æ–œè§’æ¡ä»¶: {tilt_ok} ({avg_metrics.get('tilt_angle', 'N/A'):.2f} <= {self.tilt_threshold:.2f})")
                if self.check_direction:
                    print(f"  æ–¹å‘æ¡ä»¶: {direction_ok} ({avg_metrics.get('movement_alignment', 'N/A'):.2f} >= 0.5)")
                print(f"  æ­¥æ•°æ¡ä»¶: {steps_ok} ({episode_length} <= {self.adjusted_max_steps_threshold})")
                print(f"  æ˜¯å¦æˆåŠŸ: {episode_success}")

            episode_performance_metrics["phase"] = self.phase
            for key, values in episode_metrics.items():
                if key == "current_step":
                    episode_performance_metrics[key].append(episode_length)
                elif values:
                    if key in ["tilt_angle", "distance_to_target"]:
                        episode_performance_metrics[key].append(np.mean(values))
                    elif key == "movement_alignment":
                        episode_performance_metrics[key].append(np.mean(values))
                    elif key == "direction_exploration":
                        episode_performance_metrics[key].append(np.mean(values) if values else 0)

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics = {}
            for key, values in episode_performance_metrics.items():
                if values and key != "phase":
                    self.performance_metrics[key] = np.mean(values)

            # æ›´æ–°æˆåŠŸç‡å†å²è®°å½•
            self.success_history.extend(episode_successes)
            if len(self.success_history) > self.eval_window_size:
                self.success_history = self.success_history[-self.eval_window_size:]

            history_entry = {
                "phase": self.phase,
                'timesteps': self.num_timesteps,
                'mean_reward': np.mean(episode_rewards),
                'std_reward': np.std(episode_rewards),
                'success_rate': sum(episode_successes) / len(episode_successes) if episode_successes else 0.0
            }
            history_entry.update(self.performance_metrics)
            self.evaluation_history.append(history_entry)

            if not hasattr(self, "evaluations_results"):
                self.evaluations_results = []
            if not hasattr(self, "evaluations_timesteps"):
                self.evaluations_timesteps = []

            self.performance_history.append(self.performance_metrics.copy())
            self.evaluations_results.append(episode_rewards)
            self.evaluations_timesteps.append(self.num_timesteps)

            if self.verbose > 0:
                print(f"è¯„ä¼°ç»“æœ - å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}, æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
                metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in self.performance_metrics.items()])
                print(f"æ€§èƒ½æŒ‡æ ‡: {metrics_str}")
                success_rate = sum(episode_successes) / len(episode_successes) if episode_successes else 0.0
                print(f"æˆåŠŸç‡: {success_rate:.2%} ({sum(episode_successes)}/{len(episode_successes)} å›åˆæˆåŠŸ)")

    def _on_step(self) -> bool:
        """
        åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤åè°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œè¯„ä¼°ã€‚
        è¿”å› False å°†åœæ­¢è®­ç»ƒã€‚
        """
        if self.eval_freq > 0 and self.num_timesteps % self.eval_freq == 0:
            # è¿›è¡Œè¯„ä¼°
            self._custom_evaluate()

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€§èƒ½é˜ˆå€¼
            if self.performance_thresholds:
                all_thresholds_met = True
                for metric, threshold in self.performance_thresholds.items():
                    if metric not in self.performance_metrics:
                        all_thresholds_met = False
                        if self.verbose > 0:
                            print(f"æ€§èƒ½æŒ‡æ ‡ {metric} æœªæ‰¾åˆ°ï¼Œæ— æ³•æ£€æŸ¥é˜ˆå€¼")
                        break
                    current_value = self.performance_metrics[metric]
                    if metric in ["distance_to_target", "tilt_angle", "current_step"]:
                        if current_value > threshold:
                            all_thresholds_met = False
                            if self.verbose > 0:
                                print(f"æ€§èƒ½æŒ‡æ ‡ {metric} æœªè¾¾åˆ°é˜ˆå€¼: {current_value:.2f} > {threshold:.2f}")
                    else:
                        if current_value < threshold:
                            all_thresholds_met = False
                            if self.verbose > 0:
                                print(f"æ€§èƒ½æŒ‡æ ‡ {metric} æœªè¾¾åˆ°é˜ˆå€¼: {current_value:.2f} < {threshold:.2f}")

                if all_thresholds_met:
                    if self.verbose > 0:
                        print(f"é˜¶æ®µ {self.phase} æ‰€æœ‰æ€§èƒ½é˜ˆå€¼å·²è¾¾åˆ°ï¼Œæ ‡è®°é˜¶æ®µå®Œæˆ")
                    self.phase_complete = True

            # æ£€æŸ¥æ—©åœæ¡ä»¶ - åŸºäºå¥–åŠ±æ”¹è¿›
            mean_reward = self.evaluation_history[-1]['mean_reward']
            if mean_reward > self.best_mean_reward_for_early_stop + self.min_delta:
                self.best_mean_reward_for_early_stop = mean_reward
                self.no_improvement_count = 0
                if self.verbose > 1:
                    print(f"å¥–åŠ±æ”¹è¿›: {mean_reward:.2f} > {self.best_mean_reward_for_early_stop:.2f}ï¼Œé‡ç½®æ—©åœè®¡æ•°å™¨")
            else:
                self.no_improvement_count += 1
                if self.verbose > 1:
                    print(f"å¥–åŠ±æ— æ”¹è¿›ï¼Œå½“å‰è®¡æ•°: {self.no_improvement_count}/{self.early_stop_patience}")

            # æ£€æŸ¥æˆåŠŸç‡æ¡ä»¶æ—©åœï¼ˆä¼˜å…ˆï¼‰
            success_rate_met = self._check_success_rate_condition()

            if success_rate_met:
                if self.verbose > 0:
                    print(f"æ—©åœè§¦å‘: æˆåŠŸç‡æ¡ä»¶è¿ç»­æ»¡è¶³ {self.success_early_stop_count} æ¬¡ (>= 3)")
                self.phase_complete = True
                return False

            # å¦‚æœæˆåŠŸç‡æœªæ»¡è¶³ï¼Œå†æ£€æŸ¥å…¶ä»–æ¡ä»¶
            state_conditions_met = self._check_state_conditions()
            if state_conditions_met:
                if self.verbose > 0:
                    print(f"æ—©åœè§¦å‘: çŠ¶æ€æ¡ä»¶æ»¡è¶³ (è·ç¦», å€¾æ–œè§’ç­‰æŒ‡æ ‡è¾¾åˆ°é˜ˆå€¼)")
                return False

            if self.no_improvement_count >= self.early_stop_patience:
                if self.verbose > 0:
                    print(
                        f"æ—©åœè§¦å‘: å¥–åŠ±è¿ç»­ {self.no_improvement_count} æ¬¡è¯„ä¼°æ— æ”¹è¿› (>= {self.early_stop_patience})")
                return False

            # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦æ»¡è¶³æ¡ä»¶ (å¥–åŠ±ç¨³å®šæ€§)
            if len(self.evaluation_history) > 1:
                current_std = self.evaluation_history[-1]['std_reward']
                current_mean = self.evaluation_history[-1]['mean_reward']
                if abs(current_mean) > 1e-6:
                    std_ratio = current_std / abs(current_mean)
                    if std_ratio <= self.std_threshold_ratio:
                        if self.verbose > 0:
                            print(
                                f"å¥–åŠ±ç¨³å®šæ€§è¾¾åˆ°é˜ˆå€¼: æ ‡å‡†å·®/å¹³å‡å€¼ = {std_ratio:.2f} <= {self.std_threshold_ratio:.2f}")
                        self.phase_complete = True
                    else:
                        if self.verbose > 1:
                            print(
                                f"å¥–åŠ±ç¨³å®šæ€§æœªè¾¾åˆ°é˜ˆå€¼: æ ‡å‡†å·®/å¹³å‡å€¼ = {std_ratio:.2f} > {self.std_threshold_ratio:.2f}")

        return True

    def get_phase_complete(self) -> bool:
        """æ£€æŸ¥å½“å‰é˜¶æ®µæ˜¯å¦å®Œæˆ"""
        return self.phase_complete

    def get_evaluation_history(self) -> list:
        """è·å–è¯„ä¼°å†å²è®°å½•"""
        return self.evaluation_history

    def get_performance_metrics(self) -> dict:
        """è·å–æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡"""
        return self.performance_metrics


class SaveModelLogCallback(BaseCallback):
    """åœ¨ä¿å­˜æœ€ä½³æ¨¡å‹æ—¶æ‰“å°è¯¦ç»†æ—¥å¿—çš„å›è°ƒ"""

    def __init__(self, verbose=1):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        """è¿™ä¸ªæ–¹æ³•åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œ"""
        print(f"\n{'=' * 50}")
        print(f"ğŸŒŸ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹ï¼åœ¨æ­¥æ•° {self.num_timesteps} å¤„ä¿å­˜")
        print(f"{'=' * 50}\n")
        return True


class SaveLatestModelCallback(BaseCallback):
    def __init__(self, save_path, save_freq=10000, verbose=1):
        super().__init__(verbose)
        self.save_path = save_path
        self.save_freq = save_freq

    def _on_step(self):
        if self.n_calls % self.save_freq == 0:
            self.model.save(self.save_path)
            if hasattr(self.training_env, "save"):
                self.training_env.save(f"{self.save_path}_vecnorm.pkl")
        return True


class SaveCheckpointCallback(BaseCallback):
    def __init__(self, algorithm, save_dir, save_freq=20000, max_checkpoints=5, verbose=1):
        super().__init__(verbose)
        self.algorithm = algorithm
        self.save_dir = save_dir
        self.save_freq = save_freq
        self.max_checkpoints = max_checkpoints
        self.checkpoints = []

    def _on_step(self):
        if self.n_calls % self.save_freq == 0:
            timestamp = int(time.time())
            checkpoint_path = os.path.join(
                self.save_dir,
                f"checkpoint_{self.algorithm}_{self.num_timesteps}"
            )
            self.model.save(checkpoint_path)
            if hasattr(self.training_env, "save"):
                self.training_env.save(f"{checkpoint_path}_vecnorm.pkl")

            self.checkpoints.append(checkpoint_path)

            # ç®¡ç†æ£€æŸ¥ç‚¹æ•°é‡
            if len(self.checkpoints) > self.max_checkpoints:
                old_checkpoint = self.checkpoints.pop(0)
                if os.path.exists(f"{old_checkpoint}.zip"):
                    os.remove(f"{old_checkpoint}.zip")
                if os.path.exists(f"{old_checkpoint}_vecnorm.pkl"):
                    os.remove(f"{old_checkpoint}_vecnorm.pkl")

        return True


class EarlyStoppingException(Exception):
    """å½“è®­ç»ƒåº”è¯¥æå‰åœæ­¢æ—¶æŠ›å‡ºçš„å¼‚å¸¸"""
    pass


class LinearLRDecayCallback(BaseCallback):
    """
    åŸºäºåˆå§‹å­¦ä¹ ç‡çš„çº¿æ€§è¡°å‡å›è°ƒå‡½æ•°
    """

    def __init__(self, verbose=0, decay_start_step=0, decay_end_step=None, final_lr_fraction=0.1,
                 decay_by_time_steps=10000):
        """
        å‚æ•°:
            verbose: æ—¥å¿—è¯¦ç»†ç¨‹åº¦
            decay_start_step: å¼€å§‹è¡°å‡çš„æ­¥æ•°
            decay_end_step: ç»“æŸè¡°å‡çš„æ­¥æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ€»è®­ç»ƒæ­¥æ•°
            final_lr_fraction: æœ€ç»ˆå­¦ä¹ ç‡ç›¸å¯¹äºåˆå§‹å­¦ä¹ ç‡çš„æ¯”ä¾‹
        """
        super(LinearLRDecayCallback, self).__init__(verbose)
        self.decay_start_step = decay_start_step
        self.decay_end_step = decay_end_step
        self.final_lr_fraction = final_lr_fraction
        self.decay_by_time_steps = decay_by_time_steps
        self.initial_lr = None

    def _init_callback(self) -> None:
        """åˆå§‹åŒ–å›è°ƒï¼Œè·å–åˆå§‹å­¦ä¹ ç‡"""
        if self.initial_lr is None:
            self.initial_lr = self.model.learning_rate
        self.current_lr = self.initial_lr

        # éªŒè¯æ¨¡å‹ä¼˜åŒ–å™¨ç»“æ„
        if hasattr(self.model, 'policy'):
            if hasattr(self.model.policy, 'optimizer'):
                print("æ¨¡å‹ä½¿ç”¨å•ä¸ªä¼˜åŒ–å™¨")
            elif hasattr(self.model.policy, 'actor') and hasattr(self.model.policy.actor, 'optimizer'):
                print("æ¨¡å‹ä½¿ç”¨åˆ†ç¦»çš„Actorä¼˜åŒ–å™¨")
            elif hasattr(self.model.policy, 'critic') and hasattr(self.model.policy.critic, 'optimizer'):
                print("æ¨¡å‹ä½¿ç”¨åˆ†ç¦»çš„Criticä¼˜åŒ–å™¨")
            else:
                print("è­¦å‘Š: æ— æ³•è¯†åˆ«ä¼˜åŒ–å™¨ç»“æ„")

    def _on_step(self) -> bool:
        """æ¯æ­¥æ›´æ–°å­¦ä¹ ç‡"""
        # åªåœ¨ç‰¹å®šæ­¥æ•°æ›´æ–°å­¦ä¹ ç‡ï¼Œä¾‹å¦‚æ¯1000æ­¥
        if self.decay_start_step <= self.n_calls <= self.decay_end_step and self.n_calls % self.decay_by_time_steps == 0:
            # åªæœ‰åœ¨è¡°å‡èŒƒå›´å†…æ‰è°ƒæ•´å­¦ä¹ ç‡
            if self.decay_start_step <= self.n_calls <= self.decay_end_step:
                # è®¡ç®—å½“å‰è¿›åº¦
                progress = (self.n_calls - self.decay_start_step) / (self.decay_end_step - self.decay_start_step)
                # çº¿æ€§è¡°å‡ï¼šä»åˆå§‹å€¼åˆ° final_lr_fraction * åˆå§‹å€¼
                current_lr = self.initial_lr * (1 - (1 - self.final_lr_fraction) * progress)
                # æ›´æ–°PPOä¼˜åŒ–å™¨
                if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'optimizer'):
                    for param_group in self.model.policy.optimizer.param_groups:
                        param_group['lr'] = current_lr

                # æ›´æ–°SACä¼˜åŒ–å™¨
                if hasattr(self.model, 'actor') and hasattr(self.model.actor, 'optimizer'):
                    for param_group in self.model.actor.optimizer.param_groups:
                        param_group['lr'] = current_lr

                if hasattr(self.model, 'critic') and hasattr(self.model.critic, 'optimizer'):
                    for param_group in self.model.critic.optimizer.param_groups:
                        param_group['lr'] = current_lr

                # æ›´æ–°SACæ¸©åº¦å‚æ•°ä¼˜åŒ–å™¨(å¦‚æœæœ‰)
                if hasattr(self.model, 'log_ent_coef') and hasattr(self.model,
                                                                   'ent_coef_optimizer') and self.model.ent_coef_optimizer is not None:
                    for param_group in self.model.ent_coef_optimizer.param_groups:
                        param_group['lr'] = current_lr

                # æ›´æ–°SB3çš„æ—¥å¿—å€¼ï¼ˆå°è¯•è¦†ç›–å†…éƒ¨è®°å½•çš„å­¦ä¹ ç‡ï¼‰
                if hasattr(self.model, 'logger') and self.model.logger:
                    self.model.logger.record("train/learning_rate", current_lr)
                if self.model.logger:
                    self.model.logger.record("custom/actual_learning_rate", current_lr)

                # åŒæ­¥å­¦ä¹ ç‡åˆ°æ¨¡å‹å±æ€§
                if hasattr(self.model, 'learning_rate'):
                    self.model.learning_rate = current_lr

                # æ›´æ–°æ—¥å¿—å€¼
                if hasattr(self.model, 'logger') and self.model.logger:
                    self.model.logger.record("train/learning_rate", current_lr)
                if self.model.logger:
                    self.model.logger.record("custom/actual_learning_rate", current_lr)

                self.current_lr = current_lr  # ä¿å­˜å½“å‰å­¦ä¹ ç‡åˆ°å›è°ƒå®ä¾‹ä¸­

        return True


class BallPositionCallback(BaseCallback):
    """
    ç®€åŒ–ç‰ˆæœ¬çš„å›è°ƒå‡½æ•°ï¼Œåªè®°å½•å°çƒä½ç½®åˆ†å¸ƒå¹¶è®¡ç®—ç›¸å…³æŒ‡æ ‡ï¼Œä¸è¿›è¡Œå¯è§†åŒ–
    """

    def __init__(self, eval_env, verbose=0):
        super(BallPositionCallback, self).__init__(verbose)
        self.eval_env = get_base_env(eval_env)  # è¯„ä¼°ç¯å¢ƒ
        self.ball_positions = []  # å­˜å‚¨æ‰€æœ‰è§‚å¯Ÿåˆ°çš„çƒä½ç½®
        self.position_counts = {}  # è®¡æ•°æ¯ä¸ªä½ç½®å‡ºç°çš„æ¬¡æ•°
        self.last_position = None  # è®°å½•ä¸Šä¸€æ¬¡çš„ä½ç½®

        # å°è¯•è·å–è¿·å®«å¤§å°æˆ–è®¾ç½®é»˜è®¤å€¼
        if hasattr(self.eval_env, 'maze_size'):
            self.maze_size = self.eval_env.maze_size
        else:
            self.maze_size = (7, 7)  # é»˜è®¤å€¼

    def _on_step(self):
        return True

    def _on_rollout_start(self):
        """åœ¨æ¯æ¬¡æ”¶é›†æ–°çš„rolloutå¼€å§‹æ—¶è°ƒç”¨"""
        # é‡ç½®ç¯å¢ƒå¹¶è®°å½•çƒçš„ä½ç½®
        self.eval_env.reset()

        # è·å–çƒçš„ä½ç½®
        ball_pos = self._get_ball_position()

        if ball_pos is not None:
            # æ£€æŸ¥ä½ç½®æ˜¯å¦æœ‰å˜åŒ–
            position_changed = (self.last_position != ball_pos)
            if position_changed:
                print(f"çƒä½ç½®å˜åŒ–: {self.last_position} -> {ball_pos}")

            # æ›´æ–°ä¸Šä¸€æ¬¡ä½ç½®
            self.last_position = ball_pos

            # è®°å½•ä½ç½®
            self.ball_positions.append(ball_pos)

            # æ›´æ–°ä½ç½®è®¡æ•°
            pos_key = str(ball_pos)
            if pos_key in self.position_counts:
                self.position_counts[pos_key] += 1
            else:
                self.position_counts[pos_key] = 1

            # æ¯10æ¬¡rolloutè®¡ç®—å¹¶è®°å½•æŒ‡æ ‡
            if len(self.ball_positions) % 10 == 0:
                self._calculate_metrics()

    def _get_ball_position(self):
        """å°è¯•è·å–çƒçš„ä½ç½®"""
        if hasattr(self.eval_env, 'ball_pos'):
            return self.eval_env.ball_pos
        elif hasattr(self.eval_env, 'get_ball_position'):
            return self.eval_env.get_ball_position()
        elif hasattr(self.eval_env, 'observation') and len(self.eval_env.observation) >= 5:
            # ä»28ç»´è§‚å¯Ÿç©ºé—´ä¸­æå–çƒçš„ç›¸å¯¹ä½ç½®
            dx, dy = self.eval_env.observation[3], self.eval_env.observation[4]
            robot_x, robot_y = self.eval_env.observation[0], self.eval_env.observation[1]
            # è®¡ç®—çƒçš„ç»å¯¹ä½ç½®
            return (robot_x + dx, robot_y + dy)
        return None

    def _calculate_metrics(self):
        """è®¡ç®—å¹¶è®°å½•ä½ç½®åˆ†å¸ƒæŒ‡æ ‡"""
        # åˆ›å»ºåˆ†å¸ƒçŸ©é˜µ
        distribution = np.zeros(self.maze_size)

        # å¡«å……åˆ†å¸ƒæ•°æ®
        for pos in self.ball_positions:
            x, y = pos
            if 0 <= x < self.maze_size[0] and 0 <= y < self.maze_size[1]:
                distribution[x, y] += 1

        # å½’ä¸€åŒ–åˆ†å¸ƒ
        if np.sum(distribution) > 0:
            distribution = distribution / np.sum(distribution)

        # è®¡ç®—ç†µä»¥é‡åŒ–éšæœºæ€§
        entropy = self._calculate_entropy(distribution)
        self.logger.record('environment/position_entropy', entropy)
        print(f"ä½ç½®ç†µ: {entropy:.4f}")

        # è®¡ç®—è¦†ç›–ç‡ - æœ‰å¤šå°‘å¯èƒ½çš„ä½ç½®è¢«ä½¿ç”¨äº†
        total_positions = self.maze_size[0] * self.maze_size[1]
        used_positions = len(self.position_counts)
        coverage = used_positions / total_positions
        self.logger.record('environment/position_coverage', coverage)
        print(f"ä½ç½®è¦†ç›–ç‡: {coverage:.4f} ({used_positions}/{total_positions})")

        # æ‰“å°å½“å‰è®°å½•çš„å”¯ä¸€ä½ç½®æ•°
        print(f"å·²è®°å½• {len(self.ball_positions)} ä¸ªä½ç½®æ ·æœ¬ï¼ŒåŒ…å« {used_positions} ä¸ªå”¯ä¸€ä½ç½®")

    def _calculate_entropy(self, distribution):
        """è®¡ç®—åˆ†å¸ƒçš„ç†µï¼Œé‡åŒ–éšæœºæ€§"""
        # å°†é›¶æ¦‚ç‡æ›¿æ¢ä¸ºå¾ˆå°çš„æ•°ï¼Œé¿å…log(0)
        distribution = distribution.flatten()
        distribution = distribution[distribution > 0]
        if len(distribution) == 0:
            return 0
        return -np.sum(distribution * np.log2(distribution))


class RewardMetricsCallback(BaseCallback):
    """
    å¢å¼ºç‰ˆæŒ‡æ ‡è®°å½•å›è°ƒå‡½æ•°ï¼Œé€‚é…28ç»´è§‚å¯Ÿç©ºé—´ï¼Œè®°å½•å…³é”®æŒ‡æ ‡å¹¶è½¬æ¢ä¸ºæ˜“äºç†è§£çš„å•ä½ï¼š
    - æœ€ç»ˆåˆ°è¾¾çƒçš„è·ç¦» (ç±³)
    - å€¾æ–œè§’åº¦ (åº¦)
    - åèˆªè§’ (åº¦ï¼Œ0-360èŒƒå›´)
    - å®Œæˆæ­¥æ•°
    - å°çƒåˆå§‹ä½ç½® (x, yåæ ‡)
    - ç›®æ ‡è·Ÿè¸ªæŒ‡æ ‡ (å°çƒåœ¨è§†é‡ä¸­çš„ä½ç½®)
    - æœºå™¨äººæ­£é¢ä¸çƒçš„è§’åº¦å·®å¼‚
    - æœºå™¨äººé€Ÿåº¦å’Œå¹³è¡¡çŠ¶æ€
    """

    def __init__(self,
                 verbose=1,
                 log_dir="path/to/tensorboard_logs",
                 log_freq=1000,  # é™ä½è®°å½•é¢‘ç‡
                 check_direction=False):
        """
        åˆå§‹åŒ–å›è°ƒå‡½æ•°

        å‚æ•°:
            verbose (int): æ—¥å¿—è¯¦ç»†ç¨‹åº¦
            log_dir (str): TensorBoardæ—¥å¿—ä¿å­˜ç›®å½•
            log_freq (int): è®°å½•æ—¥å¿—çš„é¢‘ç‡(æ­¥æ•°)
            check_direction (bool): æ˜¯å¦æ£€æŸ¥æ–¹å‘å¯¹é½ä½œä¸ºæˆåŠŸæ¡ä»¶
        """
        super(RewardMetricsCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.log_freq = log_freq
        self.check_direction = check_direction

        # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®
        self.episode_count = 0
        self.current_episode_length = 0
        self.episode_log_freq = 100

        # æˆåŠŸç‡ç»Ÿè®¡
        self.total_attempts = 0
        self.total_successes = 0
        self.success_history = []  # è®°å½•æ¯ä¸ªå›åˆæ˜¯å¦æˆåŠŸ
        self.success_window_size = 100  # æˆåŠŸç‡è®¡ç®—çª—å£å¤§å°
        self.success_rate = 0.0

        # å½“å‰æ­¥æ•°
        self.current_steps = 0
        # å½“å‰episodeçš„å…³é”®æŒ‡æ ‡
        self.current_distance = None
        self.current_tilt = None
        self.current_yaw = None
        self.current_angle_diff = None
        self.prev_angle_diff = None

        # æ·»åŠ ç›®æ ‡è·Ÿè¸ªæŒ‡æ ‡
        self.current_target_tracking = None
        self.current_laser_target_position = None
        self.current_front_angle_diff = None  # æœºå™¨äººæ­£é¢ä¸çƒçš„è§’åº¦å·®å¼‚

        # æ·»åŠ é€Ÿåº¦å’Œå¹³è¡¡çŠ¶æ€æŒ‡æ ‡
        self.current_forward_speed = None
        self.current_lateral_speed = None
        self.current_movement_consistency = None
        self.current_roll = None
        self.current_pitch = None
        self.current_roll_rate = None
        self.current_pitch_rate = None

        # å°çƒä½ç½® - ç°åœ¨ä¿å­˜åˆå§‹ä½ç½®
        self.initial_ball_position = None
        self.current_ball_position = None

        # å½“å‰å›åˆçš„æŒ‡æ ‡åˆ—è¡¨ï¼Œç”¨äºè®¡ç®—å¹³å‡å€¼
        self.episode_distances = []
        self.episode_tilts = []
        self.episode_movement_consistencies = []

        # å®šä¹‰å€¾æ–œçŠ¶æ€æè¿°
        self.tilt_descriptions = {
            (0, 5): "éå¸¸ç¨³å®š",
            (5, 10): "ç¨³å®š",
            (10, 15): "è½»å¾®å€¾æ–œ",
            (15, 25): "æ˜æ˜¾å€¾æ–œ",
            (25, 35): "ä¸¥é‡å€¾æ–œ",
            (35, float('inf')): "å³å°†æ‘”å€’"
        }

        # å®šä¹‰ç›®æ ‡è·Ÿè¸ªè´¨é‡æè¿°
        self.tracking_descriptions = {
            (0.8, 1.0): "å®Œç¾å±…ä¸­",
            (0.6, 0.8): "è‰¯å¥½å±…ä¸­",
            (0.4, 0.6): "éƒ¨åˆ†å±…ä¸­",
            (0.2, 0.4): "è¾¹ç¼˜å¯è§",
            (0.0, 0.2): "å‡ ä¹ä¸å¯è§"
        }

        # å®šä¹‰ç§»åŠ¨ä¸€è‡´æ€§æè¿°
        self.consistency_descriptions = {
            (0.8, 1.0): "é«˜åº¦ä¸€è‡´",
            (0.5, 0.8): "è¾ƒä¸ºä¸€è‡´",
            (0.2, 0.5): "éƒ¨åˆ†ä¸€è‡´",
            (0.0, 0.2): "ç•¥æœ‰ä¸€è‡´",
            (-0.2, 0.0): "ç•¥å¾®ä¸ä¸€è‡´",
            (-0.5, -0.2): "éƒ¨åˆ†ä¸ä¸€è‡´",
            (-0.8, -0.5): "è¾ƒä¸ºä¸ä¸€è‡´",
            (-1.0, -0.8): "é«˜åº¦ä¸ä¸€è‡´"
        }

        # æˆåŠŸæ¡ä»¶é˜ˆå€¼ï¼Œå°†åœ¨ _on_step ä¸­åŠ¨æ€æ›´æ–°
        self.success_distance_threshold = 0.5
        self.success_tilt_threshold = 0.3
        self.max_steps_threshold = 70
        self.adjusted_max_steps_threshold = self.max_steps_threshold

    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨çš„æ–¹æ³•"""
        # è·å–å½“å‰ç¯å¢ƒ
        env_ = self.training_env
        env = get_base_env(env_)

        # è·å–å½“å‰è¯¾ç¨‹é˜¶æ®µ
        current_phase = getattr(env, 'curriculum_phase', 1)

        # åŠ¨æ€åŠ è½½é…ç½®å’ŒæˆåŠŸæ¡ä»¶é˜ˆå€¼
        try:
            env_config = getattr(env, 'env_config', None)
            if env_config and 'performance_thresholds' in env_config:
                perf_thresholds = env_config['performance_thresholds']
                self.success_distance_threshold = perf_thresholds.get('distance_to_target', 0.5)
                self.success_tilt_threshold = perf_thresholds.get('tilt_angle', 0.3)
                self.max_steps_threshold = perf_thresholds.get('max_steps', 70)
            else:
                # å¦‚æœç¯å¢ƒä¸­æ²¡æœ‰é…ç½®ï¼Œå›é€€åˆ°é»˜è®¤å€¼æˆ–é…ç½®æ–‡ä»¶
                config = get_config(current_phase)
                if 'performance_thresholds' in config:
                    perf_thresholds = config['performance_thresholds']
                    self.success_distance_threshold = perf_thresholds.get('distance_to_target', 0.5)
                    self.success_tilt_threshold = perf_thresholds.get('tilt_angle', 0.3)
                    self.max_steps_threshold = perf_thresholds.get('max_steps', 70)
        except Exception as e:
            if self.verbose > 0:
                print(f"åŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼: {e}")

        # è·å–å½“å‰è§‚å¯Ÿå€¼
        if hasattr(env, 'get_raw_obs'):
            obs = env.get_raw_obs()  # è·å–åŸå§‹è§‚å¯Ÿå€¼
            obs = np.round(obs, decimals=6)
        else:
            # å¦‚æœç¯å¢ƒä¸æ”¯æŒ get_raw_obsï¼Œå›é€€åˆ°é»˜è®¤é€»è¾‘
            obs_normalized = self.locals.get('new_obs')[0]
            if self.verbose > 0:
                print(f"è­¦å‘Šï¼šç¯å¢ƒä¸æ”¯æŒ get_raw_obsï¼Œä½¿ç”¨å½’ä¸€åŒ–æ•°æ®: {obs_normalized}")
            if hasattr(self.training_env, 'obs_rms'):
                mean = self.training_env.obs_rms.mean
                var = self.training_env.obs_rms.var
                obs_ = obs_normalized * np.sqrt(var + 1e-8) + mean
                _obs = obs_.copy()
                _obs[17:25] = np.round(obs_normalized[17:25]).astype(float)
                obs = np.round(_obs, decimals=6)
            else:
                obs = obs_normalized

        self.current_episode_length += 1
        self.current_steps = self.current_episode_length
        # å°è¯•ä»ç¯å¢ƒä¸­è·å–æ­¥æ•°ä¿¡æ¯
        try:
            if hasattr(env, 'get_performance_metrics'):
                metrics = env.get_performance_metrics()
                if 'max_steps' in metrics:
                    self.current_steps = metrics.get('current_step', self.current_steps)
            elif hasattr(env, 'current_step'):
                self.current_steps = env.current_step
        except Exception as e:
            if self.verbose > 0:
                print(f"è·å–æ­¥æ•°ä¿¡æ¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ­¥æ•° {self.current_steps}")

        # æå–å…³é”®æŒ‡æ ‡ - é€‚é…28ç»´è§‚å¯Ÿç©ºé—´
        if len(obs) == 28:  # ç¡®ä¿è§‚å¯Ÿç©ºé—´ç»´åº¦æ­£ç¡®
            # æå–æœºå™¨äººä½ç½®å’Œæœå‘ (ç´¢å¼•0-2)
            robot_x, robot_y = obs[0], obs[1]
            yaw = obs[2]
            self.current_yaw = yaw

            # æå–çƒçš„ç›¸å¯¹ä½ç½® (ç´¢å¼•3-4)
            dx, dy = obs[3], obs[4]
            self.current_distance = np.sqrt(dx ** 2 + dy ** 2)  # è®¡ç®—åˆ°çƒçš„è·ç¦»
            self.current_ball_position = (dx, dy)
            self.episode_distances.append(self.current_distance)  # è®°å½•åˆ°å›åˆåˆ—è¡¨ä¸­

            # æå–å¹³è¡¡çŠ¶æ€ (ç´¢å¼•5-8)
            roll, pitch = obs[5], obs[6]
            roll_rate, pitch_rate = obs[7], obs[8]
            self.current_roll = roll
            self.current_pitch = pitch
            self.current_roll_rate = roll_rate
            self.current_pitch_rate = pitch_rate
            self.current_tilt = np.sqrt(roll ** 2 + pitch ** 2)  # è®¡ç®—å€¾æ–œè§’åº¦
            self.episode_tilts.append(self.current_tilt)  # è®°å½•åˆ°å›åˆåˆ—è¡¨ä¸­

            # è®¡ç®—æœºå™¨äººæœå‘ä¸ç›®æ ‡æ–¹å‘çš„è§’åº¦å·®
            target_angle = np.arctan2(dy, dx)
            angle_diff = np.arctan2(np.sin(target_angle - yaw), np.cos(target_angle - yaw))
            self.current_angle_diff = angle_diff

            # æ›´æ–°ä¸Šä¸€æ­¥è§’åº¦å·®
            self.prev_angle_diff = angle_diff

            # å¦‚æœæ˜¯æ–°çš„episodeçš„ç¬¬ä¸€æ­¥ï¼Œè®°å½•åˆå§‹ä½ç½®
            if self.current_episode_length == 1:
                self.initial_ball_position = self.current_ball_position

            # æå–æ¿€å…‰ä¼ æ„Ÿå™¨æ•°æ® (ç´¢å¼•9-16)å’Œç›®æ ‡ç±»å‹ (ç´¢å¼•17-24)
            laser_distances = obs[9:17]  # 8ä¸ªæ¿€å…‰æ•°æ®
            laser_targets = obs[17:25]  # 8ä¸ªæ¿€å…‰æ£€æµ‹ç›®æ ‡ç±»å‹
            # å¯»æ‰¾ç›®æ ‡ç±»å‹ä¸º2çš„ç´¢å¼•ï¼ˆè¡¨ç¤ºå°çƒï¼‰
            ball_indices = [i for i, target in enumerate(laser_targets) if int(target) == 2]
            if len(ball_indices) > 0:
                # è®¡ç®—ç›®æ ‡è·Ÿè¸ªæŒ‡æ ‡
                center_index = 3.5  # æ¿€å…‰ä¼ æ„Ÿå™¨é˜µåˆ—çš„ä¸­å¿ƒä½ç½®ï¼ˆ0-7ç´¢å¼•ç³»ç»Ÿï¼‰
                avg_deviation = sum(abs(idx - center_index) for idx in ball_indices) / len(ball_indices)
                normalized_deviation = avg_deviation / 3.5
                self.current_target_tracking = 1.0 - normalized_deviation

                # è®°å½•å°çƒåœ¨æ¿€å…‰ä¼ æ„Ÿå™¨ä¸­çš„å¹³å‡ä½ç½®
                avg_position = sum(ball_indices) / len(ball_indices)
                self.current_laser_target_position = avg_position

                # è®¡ç®—æœºå™¨äººæ­£é¢ä¸çƒçš„è§’åº¦å·®å¼‚
                laser_fov = np.pi  # 180åº¦è§†é‡èŒƒå›´ï¼ˆå¼§åº¦ï¼‰
                position_ratio = (avg_position - center_index) / 3.5  # èŒƒå›´[-1, 1]
                front_angle_diff = position_ratio * (laser_fov / 2)  # è½¬æ¢ä¸ºè§’åº¦å·®å¼‚
                self.current_front_angle_diff = front_angle_diff
            else:
                self.current_target_tracking = 0.0
                self.current_laser_target_position = None
                self.current_front_angle_diff = None

            # æå–é€Ÿåº¦å’Œç§»åŠ¨ä¸€è‡´æ€§ (ç´¢å¼•25-27)
            self.current_forward_speed = obs[25]  # å‰è¿›é€Ÿåº¦
            self.current_lateral_speed = obs[26]  # ä¾§å‘é€Ÿåº¦
            self.current_movement_consistency = obs[27]  # ç§»åŠ¨ä¸€è‡´æ€§
            self.episode_movement_consistencies.append(self.current_movement_consistency)  # è®°å½•åˆ°å›åˆåˆ—è¡¨ä¸­

        # æ£€æŸ¥episodeæ˜¯å¦ç»“æŸ
        done = env.get_done()
        # æ¯éš” log_freq æ­¥è®°å½•ä¸€æ¬¡æ•°æ®åˆ° TensorBoardï¼Œä¸”ç¡®ä¿ä¸æ˜¯ episode çš„ç¬¬ä¸€æ­¥
        if (self.log_freq is not None
                and self.num_timesteps % self.log_freq == 0 and self.current_distance is not None
                and self.current_distance < 1.0):
            # æ£€æŸ¥å…³é”®æŒ‡æ ‡ä¸­ä¸º 0 çš„æ•°é‡
            zero_count = 0
            key_metrics = [
                self.current_distance if self.current_distance is not None else 0,
                self.current_tilt if self.current_tilt is not None else 0,
                self.current_yaw if self.current_yaw is not None else 0,
                self.current_angle_diff if self.current_angle_diff is not None else 0,
                self.current_target_tracking if self.current_target_tracking is not None else 0,
                self.current_forward_speed if self.current_forward_speed is not None else 0,
                self.current_lateral_speed if self.current_lateral_speed is not None else 0,
                self.current_movement_consistency if self.current_movement_consistency is not None else 0
            ]
            for metric in key_metrics:
                if abs(metric) < 1e-6:  # è€ƒè™‘åˆ°æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼Œä½¿ç”¨ä¸€ä¸ªå¾ˆå°çš„é˜ˆå€¼æ¥åˆ¤æ–­æ˜¯å¦ä¸º 0
                    zero_count += 1

            # å¦‚æœä¸º 0 çš„æŒ‡æ ‡æ•°é‡è¶…è¿‡ 5 ä¸ªï¼Œåˆ™ä¸è®°å½•æ•°æ®
            if zero_count > 5:
                return True  # è·³è¿‡è®°å½•ï¼Œç›´æ¥è¿”å›

            # è®°å½•æ–¹å‘æ¢ç´¢è¿›å±•
            if self.prev_angle_diff is not None:
                angle_diff_change = abs(angle_diff) - abs(self.prev_angle_diff)
                self.logger.record("metrics/angle_diff_change", angle_diff_change)
            # è®°å½•å½“å‰æ­¥éª¤çš„å…³é”®æŒ‡æ ‡
            if self.current_distance is not None:
                self.logger.record("metrics/distance_to_ball", self.current_distance)
            if self.current_tilt is not None:
                tilt_degrees = np.degrees(self.current_tilt)
                self.logger.record("metrics/tilt_angle_deg", tilt_degrees)
            if self.current_yaw is not None:
                yaw_degrees = np.degrees(self.current_yaw) % 360
                self.logger.record("metrics/yaw_angle_deg", yaw_degrees)
            if self.current_angle_diff is not None:
                angle_diff_degrees = np.degrees(self.current_angle_diff)
                self.logger.record("metrics/angle_diff_deg", angle_diff_degrees)
                self.logger.record("metrics/abs_angle_diff_deg", abs(angle_diff_degrees))

            # è®°å½•ç›®æ ‡è·Ÿè¸ªæŒ‡æ ‡
            if self.current_target_tracking is not None:
                self.logger.record("metrics/target_tracking_score", self.current_target_tracking)
            if self.current_laser_target_position is not None:
                self.logger.record("metrics/laser_target_position", self.current_laser_target_position)
            if self.current_front_angle_diff is not None:
                front_angle_diff_degrees = np.degrees(self.current_front_angle_diff)
                self.logger.record("metrics/front_angle_diff_deg", front_angle_diff_degrees)
                self.logger.record("metrics/abs_front_angle_diff_deg", abs(front_angle_diff_degrees))

            # è®°å½•å°çƒå½“å‰ä½ç½®
            if self.current_ball_position is not None:
                self.logger.record("metrics/ball_position_x", self.current_ball_position[0])
                self.logger.record("metrics/ball_position_y", self.current_ball_position[1])

            # è®°å½•å¹³è¡¡çŠ¶æ€
            if self.current_roll is not None:
                self.logger.record("metrics/roll_deg", np.degrees(self.current_roll))
            if self.current_pitch is not None:
                self.logger.record("metrics/pitch_deg", np.degrees(self.current_pitch))

            # è®°å½•é€Ÿåº¦å’Œç§»åŠ¨ä¸€è‡´æ€§
            if self.current_forward_speed is not None:
                self.logger.record("metrics/forward_speed", self.current_forward_speed)
            if self.current_lateral_speed is not None:
                self.logger.record("metrics/lateral_speed", self.current_lateral_speed)
            if self.current_movement_consistency is not None:
                self.logger.record("metrics/movement_consistency", self.current_movement_consistency)

            # è®°å½•å½“å‰æ­¥æ•°
            self.logger.record("metrics/current_steps", self.current_steps)
            # è®°å½•æˆåŠŸç‡
            self.logger.record("metrics/success_rate", self.success_rate)
            # å†™å…¥æ•°æ®åˆ° TensorBoard
            self.logger.dump(self.num_timesteps)

        # åªæœ‰åœ¨ episode ç»“æŸæ—¶æ‰é‡ç½®ç›¸å…³å˜é‡å¹¶åˆ¤æ–­æˆåŠŸç‡
        if done:
            self.total_attempts += 1
            self.episode_count += 1

            # åŠ¨æ€è°ƒæ•´ max_steps_thresholdï¼Œæ£€æŸ¥ç¯å¢ƒä¸­æ˜¯å¦æœ‰ steps_per_grid å±æ€§
            if hasattr(env, 'steps_per_grid'):
                self.adjusted_max_steps_threshold = min(self.max_steps_threshold, env.steps_per_grid)
                if self.verbose > 0 and self.episode_count % self.episode_log_freq == 0:
                    print(
                        f"åŠ¨æ€è°ƒæ•´æ­¥æ•°é˜ˆå€¼: ä½¿ç”¨ {self.adjusted_max_steps_threshold} (å– {self.max_steps_threshold} å’Œç¯å¢ƒ steps_per_grid {env.steps_per_grid} çš„æœ€å°å€¼)")
            else:
                self.adjusted_max_steps_threshold = self.max_steps_threshold
                if self.verbose > 0 and self.episode_count % self.episode_log_freq == 0:
                    print(f"ç¯å¢ƒä¸­æ—  steps_per_grid å±æ€§ï¼Œä½¿ç”¨é»˜è®¤æ­¥æ•°é˜ˆå€¼: {self.adjusted_max_steps_threshold}")

            # è®¡ç®—å›åˆçš„å¹³å‡æŒ‡æ ‡
            avg_distance = np.mean(self.episode_distances) if self.episode_distances else float('inf')
            avg_tilt = np.mean(self.episode_tilts) if self.episode_tilts else float('inf')
            avg_movement_consistency = np.mean(
                self.episode_movement_consistencies) if self.episode_movement_consistencies else 0.0

            # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
            distance_ok = avg_distance <= self.success_distance_threshold
            tilt_ok = avg_tilt <= self.success_tilt_threshold
            direction_ok = True
            if self.check_direction:
                direction_ok = avg_movement_consistency >= 0.5
            steps_ok = self.current_steps <= self.adjusted_max_steps_threshold  # æ–°å¢æ­¥æ•°æ¡ä»¶

            episode_success = distance_ok and tilt_ok and direction_ok and steps_ok
            if episode_success:
                self.total_successes += 1

            # æ›´æ–°æˆåŠŸå†å²è®°å½•
            self.success_history.append(episode_success)
            if len(self.success_history) > self.success_window_size:
                self.success_history = self.success_history[-self.success_window_size:]

            # è®¡ç®—æˆåŠŸç‡
            total_episodes = len(self.success_history)
            successful_episodes = sum(self.success_history)
            self.success_rate = successful_episodes / total_episodes if total_episodes > 0 else 0.0

            # è®°å½•æˆåŠŸç‡å’Œåˆ¤æ–­ç»“æœ
            if self.verbose > 0 and self.episode_count % self.episode_log_freq == 0:
                print(
                    f"å›åˆ {self.episode_count} ç»“æŸï¼ŒæˆåŠŸç‡: {self.success_rate:.2%} ({successful_episodes}/{total_episodes})")
                print(f"æˆåŠŸæ¡ä»¶æ£€æŸ¥ï¼š")
                print(f"  è·ç¦»æ¡ä»¶: {distance_ok} ({avg_distance:.2f} <= {self.success_distance_threshold:.2f})")
                print(f"  å€¾æ–œè§’æ¡ä»¶: {tilt_ok} ({avg_tilt:.2f} <= {self.success_tilt_threshold:.2f})")
                if self.check_direction:
                    print(f"  æ–¹å‘æ¡ä»¶: {direction_ok} ({avg_movement_consistency:.2f} >= 0.5)")
                print(f"  æ­¥æ•°æ¡ä»¶: {steps_ok} ({self.current_steps} <= {self.adjusted_max_steps_threshold})")
                print(f"  æ˜¯å¦æˆåŠŸ: {episode_success}")

            # é‡ç½®å›åˆç›¸å…³å˜é‡
            self.current_episode_length = 0
            self.initial_ball_position = None  # é‡ç½®åˆå§‹ä½ç½®ï¼Œä¸ºä¸‹ä¸€ä¸ª episode åšå‡†å¤‡
            self.prev_angle_diff = None  # é‡ç½®ä¸Šä¸€æ­¥è§’åº¦å·®
            self.episode_distances = []  # é‡ç½®å›åˆæŒ‡æ ‡åˆ—è¡¨
            self.episode_tilts = []
            self.episode_movement_consistencies = []

        return True
